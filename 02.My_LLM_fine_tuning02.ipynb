{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 먼저 파인튜닝된 모델을 저장할 허깅페이스 레파지토리 2개를 만들어주어야 한다.\n",
    "\n",
    "https://huggingface.co/\n",
    "\n",
    "자기 계정에서 프로필 아이콘을 누르면 + New Model이 있다\n",
    "\n",
    "모델을 저장할 레파지토리를 만들자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파인튜닝한 모델을 허깅페이스에 저장하기 위한 개인 허깅페이스 토큰 정하기 \n",
    "huggingface_token = \"xxxx\"\n",
    "\n",
    "# 파인튜닝한 모델을 허깅페이스에 저장하기 위한 레파지토리 이름 \n",
    "huggingface_repo = 'dacorn_llm'\n",
    "\n",
    "# 파인튜닝한 나만의 모델 이름을 저장함\n",
    "my_model_name = \"Llama-3.1-Ko-Instruct-8B-dacorn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 구글 드라이브 마운트\n",
    "\n",
    "먼저, 구글 드라이브를 사용하기 위해 마운트합니다. Colab을 사용하지 않는 경우, 이 과정은 건너뛰시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir('/content/drive/MyDrive/Colab Notebooks')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모듈 설치하기\n",
    "\n",
    "미설치된 모듈이 있을 경우, pip install 명령어를 통해 필요한 모듈을 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U datasets\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U accelerate\n",
    "#!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U trl\n",
    "\n",
    "# Unsloth는 Llama 모델의 fine-tuning을 더 쉽고 효율적으로 만들어주는 도구이다.\n",
    "# [colab-new] 부분은 Colab 환경에 맞는 버전을 자동으로 설치하게 해준다.\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# fine-tuning에 필요한 추가 라이브러리들을 설치한다.\n",
    "# --no-deps 옵션은 이 라이브러리들의 종속성을 설치하지 않도록 한다.\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "!pip install --pre -U xformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "설치되는 라이브러리들:\n",
    "\n",
    "- xformers: 트랜스포머 모델의 성능을 향상시키는 라이브러리\n",
    "- trl: 강화학습을 이용한 언어 모델 훈련을 위한 라이브러리\n",
    "- peft: 매개변수 효율적 미세조정 (Parameter-Efficient Fine-Tuning)을 위한 라이브러리\n",
    "- accelerate: 딥러닝 모델의 훈련을 가속화하는 라이브러리\n",
    "- bitsandbytes: 모델 양자화를 위한 라이브러리\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모듈을 import합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TextStreamer,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    PeftConfig,\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 및 토크나이저 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"sh2orc/Llama-3.1-Korean-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4배 빠른 다운로드와 메모리 부족 문제를 방지하기 위해 지원하는 4bit 사전 양자화 모델입니다.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-it-bnb-4bit\",  # Gemma 7b의 Instruct 버전\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2b-it-bnb-4bit\",  # Gemma 2b의 Instruct 버전\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",  # Llama-3 8B\n",
    "]  # 더 많은 모델은 https://huggingface.co/unsloth 에서 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# 최대 시퀀스 길이를 설정합니다. 내부적으로 RoPE 스케일링을 자동으로 지원합니다!\n",
    "max_seq_length = 4096  \n",
    "# 자동 감지를 위해 None을 사용합니다. Tesla T4, V100은 Float16, Ampere+는 Bfloat16을 사용하세요.\n",
    "dtype = None\n",
    "# 메모리 사용량을 줄이기 위해 4bit 양자화를 사용합니다. False일 수도 있습니다.\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL,          # 베이스모델이 될 모델 -> 우리는 Llama3.1\n",
    "    max_seq_length=max_seq_length,  # 최대 시퀀스 길이를 설정합니다.\n",
    "    dtype=dtype,                    # 데이터 타입을 설정합니다.\n",
    "    load_in_4bit=load_in_4bit,      # 4bit 양자화 로드 여부를 설정합니다.\n",
    "    # token = \"hf_...\", # 게이트된 모델을 사용하는 경우 토큰을 사용하세요. 예: meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 LoRA 어댑터를 추가하여 모든 파라미터 중 단 1% ~ 10%의 파라미터만 업데이트하면 됩니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 베이스 모델 추론 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '2024년 중앙정부 재정체계는 어떻게 구성되어 있나요?'\n",
    "\n",
    "# 텍스트 생성을 위한 파이프라인 설정\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256) # max_new_tokens: 생성할 최대 토큰 수\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    do_sample=True, # 샘플링 전략 사용. 확률 분포를 기반으로 다음 토큰을 선택\n",
    "    temperature=0.2, # 샘플링의 다양성을 조절하는 파라미터. 값이 높을수록 랜덤성 증가\n",
    "    top_k=50, # 다음 토큰을 선택할 때 상위 k개의 후보 토큰 중에서 선택. 여기에서는 상위 50개의 후보 토큰 중에서 샘플링\n",
    "    top_p=0.95, # 누적 확률이 p가 될 때까지 후보 토큰을 포함\n",
    "    repetition_penalty=1.2, # 반복 패널티를 적용하여 같은 단어나 구절이 반복되는 것 방지\n",
    "    add_special_tokens=True # 모델이 입력 프롬프트의 시작과 끝을 명확히 인식할 수 있도록 특별 토큰 추가\n",
    ")\n",
    "\n",
    "# 입력 프롬프트 이후에 생성된 텍스트만 출력\n",
    "print(outputs[0][\"generated_text\"][len(prompt):]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그럴듯한 문장을 생성하긴 하지만, 여러 관련 문장을 짜깁기한 형태이고 우리가 의도한 답변 형식은 아닌 것처럼 보입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. 준비된 학습 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "허깅페이스에 등록한 데이콘 데이터를 불러오기 위해 데이터 레파지토리 이름을 정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_huggingface_name = \"HueyWoo\"\n",
    "data_repo = f\"{my_huggingface_name}/for_dacorn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpaca 형식의 프롬프트 템플릿을 정의\n",
    "alpaca_prompt = \"\"\"\n",
    "    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    {}\n",
    "    ### Input:\n",
    "    {}\n",
    "\n",
    "    ### Response:\n",
    "    {}\n",
    "    \"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋의 각 예제를 Alpaca형식으로 포매팅하는 함수를 정의\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 load\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(data_repo, split = \"train\")\n",
    "# 데이터셋에 프롬프트 적용\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬에 데이터셋 저장\n",
    "dataset.save_to_disk(\"Datasets/dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 불러오기\n",
    "dataset_path = \"Datasets/dataset\"\n",
    "dataset = load_from_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. 모델 훈련하기 - QLoRA로 파인튜닝하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLoRA 설정\n",
    "\n",
    "FastLanguageModel을 사용하여 특정 모듈에 대한 성능 향상 기법을 적용한 모델을 구성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,               # 0보다 큰 어떤 숫자도 선택 가능! 8, 16, 32, 64, 128이 권장됩니다.\n",
    "    lora_alpha=32,      # LoRA 알파 값을 설정합니다.\n",
    "    lora_dropout=0.05,  # 드롭아웃을 지원합니다.\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # 타겟 모듈을 지정합니다.\n",
    "    bias=\"none\",  # 바이어스를 지원합니다.\n",
    "    # True 또는 \"unsloth\"를 사용하여 매우 긴 컨텍스트에 대해 VRAM을 30% 덜 사용하고, 2배 더 큰 배치 크기를 지원합니다.\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=123,  # 난수 상태를 설정합니다.\n",
    "    use_rslora=False,  # 순위 안정화 LoRA를 지원합니다.\n",
    "    loftq_config=None,  # LoftQ를 지원합니다.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FastLanguageModel.get_peft_model` \n",
    "- 함수를 호출하여 모델을 초기화하고, 성능 향상을 위한 여러 파라미터를 설정합니다.\n",
    "\n",
    "`r` \n",
    "\n",
    "- 파라미터를 통해 성능 향상 기법의 강도를 조절합니다. 권장 값으로는 8, 16, 32, 64, 128 등이 있습니다.\n",
    "\n",
    "`target_modules` \n",
    "- 리스트에는 성능 향상을 적용할 모델의 모듈 이름들이 포함됩니다.\n",
    "\n",
    "`lora_alpha`와 `lora_dropout`\n",
    "- LoRA(Low-Rank Adaptation) 기법의 세부 파라미터를 조정합니다.\n",
    "\n",
    "`bias` \n",
    "- 옵션을 통해 모델의 바이어스 사용 여부를 설정할 수 있으며, 최적화를 위해 \"none\"으로 설정하는 것이 권장됩니다.\n",
    "\n",
    "`use_gradient_checkpointing` \n",
    "- 옵션을 \"unsloth\"로 설정하여 VRAM 사용량을 줄이고, 더 큰 배치 크기로 학습할 수 있도록 합니다.\n",
    "\n",
    "`use_rslora` \n",
    "- 옵션을 통해 Rank Stabilized LoRA를 사용할지 여부를 결정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 출력 결과와 같이, LoRA 가중치 행렬의 rank인 r을 작은 값으로 설정하면 학습 파라미터의 수를 크게 줄일 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 훈련하기\n",
    "\n",
    "이제 Huggingface TRL의 `SFTTrainer`를 사용해 봅시다!\n",
    "\n",
    "- 참고 문서: [TRL SFT 문서](https://huggingface.co/docs/trl/sft_trainer)\n",
    "\n",
    "훈련 옵션을 정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이콘 훈련데이터를 제대로 학습시키려면 MAX_STEP이 못해도 350이상이어야함\n",
    "EPOCH=3\n",
    "MAX_STEP=500\n",
    "lr_scheduler_type = \"cosine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "tokenizer.padding_side = \"right\"  # 토크나이저의 패딩을 오른쪽으로 설정합니다.\n",
    "\n",
    "\n",
    "train_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=2,              # 각 디바이스당 훈련 배치 크기\n",
    "    gradient_accumulation_steps=4,              # 그래디언트 누적 단계\n",
    "    warmup_steps=5,                             # 웜업 스텝 수\n",
    "    num_train_epochs=EPOCH,                     # 훈련 에폭 수\n",
    "    max_steps=MAX_STEP,                         # 최대 스텝 수\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=1,                            # logging 스텝 수\n",
    "    learning_rate=2e-4,                         # 학습률\n",
    "    fp16=not torch.cuda.is_bf16_supported(),    # fp16 사용 여부, bf16이 지원되지 않는 경우에만 사용\n",
    "    bf16=torch.cuda.is_bf16_supported(),        # bf16 사용 여부, bf16이 지원되는 경우에만 사용\n",
    "    optim=\"adamw_8bit\",                         # 최적화 알고리즘\n",
    "    weight_decay=0.01,                          # 가중치 감소\n",
    "    lr_scheduler_type=lr_scheduler_type,        # 학습률 스케줄러 유형 linear  cosine\n",
    "    seed=123,                                   # 랜덤 시드\n",
    "    output_dir=\"outputs\", \n",
    ")\n",
    "\n",
    "\n",
    "# SFTTrainer를 사용하여 모델 학습 설정\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                                    # 학습할 모델\n",
    "    tokenizer=tokenizer,                            # 토크나이저\n",
    "    train_dataset=dataset,                          # 학습 데이터셋\n",
    "    # eval_dataset=dataset,                           # 평가 데이터셋\n",
    "    dataset_text_field=\"text\",                      # 데이터셋에서 텍스트 필드의 이름\n",
    "    max_seq_length=max_seq_length,                  # 최대 시퀀스 길이\n",
    "    dataset_num_proc=2,                             # 데이터 처리에 사용할 프로세스 수\n",
    "    packing=False,                                  # 짧은 시퀀스에 대한 학습 속도를 5배 빠르게 할 수 있음\n",
    "    args=train_args\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 실습은 Colab에서 진행되므로, 제한된 런타임 내에서 간단히 학습 후 결과를 확인하기 위해 학습 단계(max_steps)는 500으로 설정했습니다. 리소스가 충분한 분들은 epoch 단위로 값을 설정하여 진행해보시기 바랍니다.\n",
    "\n",
    " \n",
    "\n",
    "학습을 수행합니다.\n",
    "\n",
    "못해도 0.1까지는 떨어트려야 조금 봐줄만한 성능이 나옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "trainer_stats = trainer.train()  # 모델을 훈련시키고 통계를 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정상적으로 학습이 진행된다면, 아래와 같이 단계별로 training loss가 출력될 것입니다.\n",
    "\n",
    "학습이 완료되면, QLoRA 모델을 로컬에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINETUNED_MODEL = \"llama31_qlora\"\n",
    "trainer.model.save_pretrained(FINETUNED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저장된 모델 가중치 파일의 크기를 확인해 보시면, MB 단위로 매우 작습니다. 이는 모델의 전체 가중치가 아닌, QLoRA가 적용된 가중치만 저장된 것이기 때문입니다.\n",
    "\n",
    "**따라서 추론 시에는 베이스 모델과 QLoRA 모델을 결합하여 하나의 모델로 만든 후 사용해야 합니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. QLoRA 파인튜닝 모델 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QLoRA로 파인튜닝된 모델을 테스트해보겠습니다. 과연 질문에 답변을 잘할 수 있을까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "\n",
    "class StopOnToken(StoppingCriteria):\n",
    "    def __init__(self, stop_token_id):\n",
    "        self.stop_token_id = stop_token_id  # 정지 토큰 ID를 초기화합니다.\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        return (\n",
    "            self.stop_token_id in input_ids[0]\n",
    "        )  # 입력된 ID 중 정지 토큰 ID가 있으면 정지합니다.\n",
    "\n",
    "\n",
    "# end_token을 설정\n",
    "stop_token = \"<|end_of_text|>\"  # end_token으로 사용할 토큰을 설정합니다.\n",
    "stop_token_id = tokenizer.encode(stop_token, add_special_tokens=False)[\n",
    "    0\n",
    "]  # end_token의 ID를 인코딩합니다.\n",
    "\n",
    "# Stopping criteria 설정\n",
    "stopping_criteria = StoppingCriteriaList(\n",
    "    [StopOnToken(stop_token_id)]\n",
    ")  # 정지 조건을 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '2024년 중앙정부 재정체계는 어떻게 구성되어 있나요?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"질문의 핵심만 파악하여 간결하게 1-2문장으로 답변하고, 불필요한 설명은 피하며 요구된 정보만 제공하세요.\", # instruction\n",
    "        question, # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=2048,  # 최대 생성 토큰 수를 설정합니다.\n",
    "    stopping_criteria=stopping_criteria  # 생성을 멈출 기준을 설정합니다.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. 모델 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 로컬에 저장합니다.\n",
    "model.save_pretrained(my_model_name)  \n",
    "\n",
    "# 모델을 허깅페이스에 저장 \n",
    "model.push_to_hub(f\"{my_huggingface_name}/{huggingface_repo}\", token = huggingface_token) # 모델을 온라인 허브에 저장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종 파인튜닝 모델 추론 테스트\n",
    "\n",
    "baseline 모델과 파인튜닝한 모델 병합이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline 모델과 파인튜닝한 모델 병합\n",
    "\n",
    "\n",
    "save_method = (\n",
    "    \"merged_16bit\"  # \"merged_4bit\", \"merged_4bit_forced\", \"merged_16bit\", \"lora\"\n",
    ")\n",
    "\n",
    "model.save_pretrained_merged(\n",
    "    BASE_MODEL,\n",
    "    tokenizer,\n",
    "    save_method=save_method,  # 저장 방식을 16비트 병합으로 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종 모델 HuggingFace에 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hub에 업로드\n",
    "model.push_to_hub_merged(\n",
    "    f\"{my_huggingface_name}/{huggingface_repo}\",\n",
    "    tokenizer,\n",
    "    save_method=save_method,\n",
    "    token=huggingface_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GGUF로 변환 !!중요!!\n",
    "\n",
    "Unsloth 는 `llama.cpp`를 복제하고 기본적으로 `q8_0`에 저장합니다. `q4_k_m`과 같은 모든 메소드를 사용할 수 있습니다.\n",
    "\n",
    "로컬 저장을 위해서는 `save_pretrained_gguf`를 사용하고, HF에 업로드하기 위해서는 `push_to_hub_gguf`를 사용하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace 허브에 업로드\n",
    "\n",
    "\n",
    "지원되는 몇 가지 양자화 방법들(전체 목록은 우리의 [위키 페이지](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)에서 확인 가능):\n",
    "\n",
    "- `q8_0` - 빠른 변환. 높은 자원 사용이지만 일반적으로 수용 가능합니다.\n",
    "- `q4_k_m` - 추천됩니다. attention.wv와 feed_forward.w2 텐서의 절반에 Q6_K를 사용하고, 나머지는 Q4_K를 사용합니다.\n",
    "- `q5_k_m` - 추천됩니다. attention.wv와 feed_forward.w2 텐서의 절반에 Q6_K를 사용하고, 나머지는 Q5_K를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization 방식 설정\n",
    "quantization_method = \"q8_0\"  # \"f16\" \"q8_0\" \"q4_k_m\" \"q5_k_m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hub 에 GGUF 업로드\n",
    "model.push_to_hub_gguf(\n",
    "    huggingface_repo + \"-gguf\",\n",
    "    tokenizer,\n",
    "    quantization_method=quantization_method,\n",
    "    token=huggingface_token,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
